{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase 1 - NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMb1Hz5dW+8xM128+hmzu/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pascuapablo/CEIA/blob/master/scripts/NLP/Clase_1_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSVzNP7eOER"
      },
      "source": [
        "documents = [\n",
        "             \"A corpus may contain texts in a single language monolingual corpus or text data in multiple languages multilingual corpus\",\n",
        "             \"In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation\",\n",
        "             \"An example of annotating a corpus is part-of-speech tagging, or POS-tagging, in which information about each word's part of speech verb, noun, adjective, etc is added to the corpus in the form of tags\",\n",
        "             \"Another example is indicating the lemma base form of each word\",\n",
        "             \"When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual\",\n",
        "             \"Some corpora have further structured levels of analysis applied\",\n",
        "             \"In particular, a number of smaller corpora may be fully parsed\",\n",
        "             \"Such corpora are usually called Treebanks or Parsed Corpora\",\n",
        "             \"The difficulty of ensuring that the entire corpus is completely and consistently annotated means that these corpora are usually smaller, containing around one to three million words\",\n",
        "             \"Other levels of linguistic structured analysis are possible, including annotations for morphology, semantics and pragmatics\",\n",
        "             \"Corpora are the main knowledge base in corpus linguistics\",\n",
        "             \"Other notable areas of application include:\",\n",
        "             \"Language technology, natural language processing, computational linguistics\",\n",
        "             \"The analysis and processing of various types of corpora are also the subject of much work in computational linguistics, speech recognition and machine translation, where they are often used to create hidden Markov models for part of speech tagging and other purposes\",\n",
        "             \"Corpora and frequency lists derived from them are useful for language teaching\",\n",
        "             \"Corpora can be considered as a type of foreign language writing aid as the contextualised grammatical knowledge acquired by non-native language users through exposure to authentic texts in corpora allows learners to grasp the manner of sentence formation in the target language, enabling effective writing\",\n",
        "             \"Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora\",\n",
        "             \"There are two main types of parallel corpora which contain texts in two languages\",\n",
        "             \"In a translation corpus, the texts in one language are translations of texts in the other language\",\n",
        "             \"In a comparable corpus, the texts are of the same kind and cover the same content, but they are not translations of each other\",\n",
        "             \"To exploit a parallel text, some kind of text alignment identifying equivalent text segments phrases or sentences is a prerequisite for analysis\",\n",
        "             \"Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus\",\n",
        "             \"Text corpora are also used in the study of historical documents, for example in attempts to decipher ancient scripts, or in Biblical scholarship\",\n",
        "             \"Some archaeological corpora can be of such short duration that they provide a snapshot in time\",\n",
        "             \"One of the shortest corpora in time may be the 15–30 year Amarna letters texts 1350 BC\",\n",
        "             \"The corpus of an ancient city, for example the Kültepe Texts of Turkey, may go through a series of corpora, determined by their find site dates\"\n",
        "             ]\n",
        "\n",
        "\n",
        "\n",
        "allwords = {}\n",
        "for d in documents:\n",
        "  document_words = d.split()\n",
        "  for w in document_words:\n",
        "    if w in allwords:\n",
        "      allwords[w] += 1 \n",
        "    else:\n",
        "      allwords[w] = 1\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFEP2UWVkn6M"
      },
      "source": [
        "### One hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMMeqyMEiTum",
        "outputId": "eb0bf591-4e3c-4efc-dc73-6858e20bdd9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "onehot = np.empty(shape = ( len(documents), len(allwords) ))\n",
        "\n",
        "for i,d in enumerate(documents):\n",
        "  for j,w in enumerate(allwords):\n",
        "    if w in d:\n",
        "      onehot[i,j] = 1\n",
        "    else:\n",
        "      onehot[i,j] = 0\n",
        "onehot\n",
        "\n",
        "      \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 1., ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDx8Z8tfkvd9"
      },
      "source": [
        "### Frecuencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZEF4ns9kyi9",
        "outputId": "ead1955a-e823-4841-a211-b5ec8dab147e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "shape = ( len(documents), len(allwords) )\n",
        "\n",
        "frecMatrix = np.empty(shape = shape)\n",
        "sparse_frecMatrix = sp.dok_matrix(shape, dtype=np.float32)\n",
        "\n",
        "for i,d in enumerate(documents):\n",
        "  for j,w in enumerate(allwords):\n",
        "    count = d.count(w)\n",
        "    frecMatrix[i,j] = count\n",
        "\n",
        "    if count is not 0:\n",
        "      sparse_frecMatrix[i,j]= count\n",
        "    \n",
        "    \n",
        "\n",
        "sparse_frecMatrix=sparse_frecMatrix.tocsr()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 259)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOFXB8vum2WO"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSytBi25m347",
        "outputId": "221fa4a9-e2fd-4df5-f34b-736c9207bf57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "idf = np.zeros(shape=(1,len(allwords)))\n",
        "n_documents = len(documents)\n",
        "for i,word  in enumerate(allwords):\n",
        "  idf[0,i]=( np.log( n_documents / allwords[word] ))\n",
        "\n",
        "\n",
        "TFIDF = frecMatrix * idf\n",
        "print(TFIDF.shape)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26, 259)\n",
            "(1, 259)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycksJbafqF7P"
      },
      "source": [
        "### Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9PWpOJDzsOP",
        "outputId": "c325d3e1-5587-48cb-c89f-bb179770a366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "TFIDF_sparse.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 259)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McPu8Cz2qJY1",
        "outputId": "53922b4e-8c03-4fff-dce8-a6b7b2421b72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "document_index = 3\n",
        "\n",
        "def cosine_similarity(TFIDF, document_index):\n",
        "  n_documents = TFIDF.shape[0]\n",
        "  similarity_index = np.zeros(shape=(n_documents))\n",
        "  for i in range(n_documents):\n",
        "    doc1 = TFIDF[document_index,:];\n",
        "    doc2 =  TFIDF[i,:]\n",
        "    a = doc1 @ doc2.T\n",
        "    b = np.linalg.norm(doc1) * np.linalg.norm(doc2)\n",
        "\n",
        "    similarity_index[i]= a / b\n",
        "  \n",
        "  return similarity_index\n",
        "\n",
        "\n",
        "similarity_index = cosine_similarity(TFIDF, 3)\n",
        "similarity_index\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.33111208, 0.32233183, 0.48628487, 1.        , 0.25399096,\n",
              "       0.21882117, 0.20654281, 0.30420534, 0.30201314, 0.2203712 ,\n",
              "       0.39020888, 0.22734686, 0.1349269 , 0.24009995, 0.25196808,\n",
              "       0.32639104, 0.36346195, 0.21768334, 0.1814174 , 0.27093073,\n",
              "       0.21606552, 0.23833788, 0.29785281, 0.24336948, 0.26110766,\n",
              "       0.22583257])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}